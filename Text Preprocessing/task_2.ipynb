{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Generate Sparse Representations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Student Name: Vipul Krishnan Muralee Dharan\n",
    "#### Student ID: 28104641\n",
    "\n",
    "Date: 02/06/2018\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6.1 and Anaconda 4.3.21 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* bs4 v4.6.0(for xml retrievel)\n",
    "* os (comes with python) (for retrieving file list in the folder)\n",
    "* re v2.2.1(regular ecpression operations)\n",
    "* nltk v3.2.3 (for using regular expression tokenizer)\n",
    "* Counter (Comes in python collection)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "The aim of this task is to build sparse representations for the meeting transcripts generated in task 1, which includes word tokenization, vocabulary generation, and the generation of sparse representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup is used for retrieving xml (Crummy, 2018). RegexpTokenizer and re are used for regluar expression tokenization (NLTK, 2018). os is used for finding the files inside a folder (Python Software Foundation, 2018). Counter is used for finding the count of each word in a paragraph (Python Software Foundation, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Unigram Vocabulary File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first save all the text files as a dictionary, so that we dont need to open the files multiple times. The key of the dictonary is the path of the file and the value is the content of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the path to text file folder\n",
    "txt_file_path = \"./txt_files\"\n",
    "\n",
    "# dict for storing the file contents\n",
    "vocD = {}\n",
    "\n",
    "# for eachc file\n",
    "for tfile in os.listdir(txt_file_path): \n",
    "    tfile = os.path.join(txt_file_path, tfile)\n",
    "    # .. exclude the sample file and some unknown file in case if it is there\n",
    "    if tfile not in ['./txt_files\\.DS_Store', './txt_files\\example_output.txt']:\n",
    "        # ..store thec content of the file into the dict\n",
    "        vocD[tfile] = open(tfile,'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect the whole words together in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all word list\n",
    "allWordList = []\n",
    "\n",
    "# obtaining the stopwords from the given files\n",
    "stopwords = open('./stopwords_en.txt','r').read()\n",
    "stop_word_list = stopwords.split('\\n')\n",
    "\n",
    "# dictionary similar to vocD, but the value is an array of words\n",
    "# defined for future use\n",
    "vocListD = {}\n",
    "\n",
    "# defining tokenizer\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "\n",
    "# for each file texts (the keys in the dict represents different files)\n",
    "for k in vocD.keys():\n",
    "    # storing the text in a separate variable\n",
    "    text = vocD[k]\n",
    "    # making all lower case\n",
    "    text = text.lower()\n",
    "    # tokenizing \n",
    "    wList = tokenizer.tokenize(text)\n",
    "    # checks for stop words and words less than length of 2\n",
    "    wList = [w for w in wList if (w not in stop_word_list) and (len(w)>2)]\n",
    "    vocListD[k] = wList\n",
    "    # appending to the main list    \n",
    "    allWordList = allWordList + wList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the words unique and introduce the document frequency constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making the elements in the list unique\n",
    "allWordList = list(set(allWordList))\n",
    "\n",
    "finalList = []\n",
    "\n",
    "# for each element in the list\n",
    "for word in allWordList:\n",
    "    count = 0\n",
    "    # checks for document frequency contraint\n",
    "    # finds the document frequency as count\n",
    "    for k in vocD.keys():\n",
    "        if word in vocListD[k]:\n",
    "            count = count + 1\n",
    "    # if it is less than 133, added to new list\n",
    "    if count <= 132:\n",
    "        finalList.append(word)\n",
    "        \n",
    "# this cell takes 1 to 1.5 minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add this to the file in the required format after arranging in alphabetic order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sorting\n",
    "sortedList = sorted(list(set(finalList)))\n",
    "\n",
    "# opening fike\n",
    "f = open('vocab.txt','w')\n",
    "\n",
    "# for each word index\n",
    "for i in range(len(sortedList)):\n",
    "    # write word and word index\n",
    "    f.write(sortedList[i]+\":\"+str(i)+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating Topic Boundary Encoded File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a dictionary where the key is same as the key of the vocD (the file paths), and the values are the vecctor showing the segment breaks and topic breaks as the specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializing dictionary\n",
    "vecD = {}\n",
    "\n",
    "# for each entries in vocD\n",
    "for k in vocD.keys():\n",
    "    doc = vocD[k]\n",
    "    vector = []\n",
    "    # for each word in the text\n",
    "    for l in range(len(doc)):\n",
    "        # if it is a new line\n",
    "        if doc[l] == '\\n':\n",
    "            # if it is the end of the text, append 0\n",
    "            if l+1 == len(doc):\n",
    "                vector.append('0')\n",
    "            # else if it is a topic start, do nothing\n",
    "            elif doc[l-1] == '*':\n",
    "                pass\n",
    "            # else if it is a topic end, add one\n",
    "            elif doc[l+1] == '*':\n",
    "                vector.append('1')\n",
    "            # else append 0\n",
    "            else:\n",
    "                vector.append('0')\n",
    "    # add the vector to vecD dict\n",
    "    vecD[k] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add this to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open the file\n",
    "f = open('topic_segs.txt','w')\n",
    "\n",
    "# for each entry in the vecD dict\n",
    "for k in vecD.keys():\n",
    "    # the line string to be written\n",
    "    toWrite = k.split('\\\\')[-1].split('.')[0]+\":\"\n",
    "    \n",
    "    # fixing the comma problem. Comma shouldnt be there at the end of the line\n",
    "    for i in range(len(vecD[k])):\n",
    "        if i != len(vecD[k]) - 1:\n",
    "            toWrite = toWrite + vecD[k][i] + \",\"\n",
    "        else:\n",
    "            toWrite = toWrite + vecD[k][i] + \"\\n\"\n",
    "    # writing to file\n",
    "    f.write(toWrite)\n",
    "# closing the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating the Sparse Files\n",
    "\n",
    "The below code generates the sparse files. The code takes each file's text, each paragraph in the text and tokenizes it. Then for each word, it checks if the word exists in the vocabulary. If yes it adds the word id with count.\n",
    "\n",
    "Note: for this task, it is assumed that the paragraph mentioned in the question means the paragraphs in the txt file and it is independent of the segment.xml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./txt_files\\ES2002a.txt : Done!\n",
      "./txt_files\\ES2002b.txt : Done!\n",
      "./txt_files\\ES2002c.txt : Done!\n",
      "./txt_files\\ES2002d.txt : Done!\n",
      "./txt_files\\ES2003a.txt : Done!\n",
      "./txt_files\\ES2003b.txt : Done!\n",
      "./txt_files\\ES2003c.txt : Done!\n",
      "./txt_files\\ES2003d.txt : Done!\n",
      "./txt_files\\ES2004a.txt : Done!\n",
      "./txt_files\\ES2004b.txt : Done!\n",
      "./txt_files\\ES2004c.txt : Done!\n",
      "./txt_files\\ES2004d.txt : Done!\n",
      "./txt_files\\ES2005a.txt : Done!\n",
      "./txt_files\\ES2005b.txt : Done!\n",
      "./txt_files\\ES2005c.txt : Done!\n",
      "./txt_files\\ES2005d.txt : Done!\n",
      "./txt_files\\ES2006a.txt : Done!\n",
      "./txt_files\\ES2006b.txt : Done!\n",
      "./txt_files\\ES2006d.txt : Done!\n",
      "./txt_files\\ES2007a.txt : Done!\n",
      "./txt_files\\ES2007b.txt : Done!\n",
      "./txt_files\\ES2007c.txt : Done!\n",
      "./txt_files\\ES2007d.txt : Done!\n",
      "./txt_files\\ES2008a.txt : Done!\n",
      "./txt_files\\ES2008b.txt : Done!\n",
      "./txt_files\\ES2008c.txt : Done!\n",
      "./txt_files\\ES2008d.txt : Done!\n",
      "./txt_files\\ES2009a.txt : Done!\n",
      "./txt_files\\ES2009b.txt : Done!\n",
      "./txt_files\\ES2009c.txt : Done!\n",
      "./txt_files\\ES2009d.txt : Done!\n",
      "./txt_files\\ES2010a.txt : Done!\n",
      "./txt_files\\ES2010b.txt : Done!\n",
      "./txt_files\\ES2010c.txt : Done!\n",
      "./txt_files\\ES2010d.txt : Done!\n",
      "./txt_files\\ES2011a.txt : Done!\n",
      "./txt_files\\ES2011b.txt : Done!\n",
      "./txt_files\\ES2011c.txt : Done!\n",
      "./txt_files\\ES2011d.txt : Done!\n",
      "./txt_files\\ES2012a.txt : Done!\n",
      "./txt_files\\ES2012b.txt : Done!\n",
      "./txt_files\\ES2012c.txt : Done!\n",
      "./txt_files\\ES2012d.txt : Done!\n",
      "./txt_files\\ES2013a.txt : Done!\n",
      "./txt_files\\ES2013b.txt : Done!\n",
      "./txt_files\\ES2013c.txt : Done!\n",
      "./txt_files\\ES2013d.txt : Done!\n",
      "./txt_files\\ES2014a.txt : Done!\n",
      "./txt_files\\ES2014b.txt : Done!\n",
      "./txt_files\\ES2014c.txt : Done!\n",
      "./txt_files\\ES2014d.txt : Done!\n",
      "./txt_files\\ES2015a.txt : Done!\n",
      "./txt_files\\ES2015d.txt : Done!\n",
      "./txt_files\\ES2016a.txt : Done!\n",
      "./txt_files\\ES2016b.txt : Done!\n",
      "./txt_files\\ES2016c.txt : Done!\n",
      "./txt_files\\ES2016d.txt : Done!\n",
      "./txt_files\\IB4003.txt : Done!\n",
      "./txt_files\\IB4005.txt : Done!\n",
      "./txt_files\\IB4010.txt : Done!\n",
      "./txt_files\\IB4011.txt : Done!\n",
      "./txt_files\\IS1000a.txt : Done!\n",
      "./txt_files\\IS1000b.txt : Done!\n",
      "./txt_files\\IS1000c.txt : Done!\n",
      "./txt_files\\IS1000d.txt : Done!\n",
      "./txt_files\\IS1001a.txt : Done!\n",
      "./txt_files\\IS1001b.txt : Done!\n",
      "./txt_files\\IS1001c.txt : Done!\n",
      "./txt_files\\IS1001d.txt : Done!\n",
      "./txt_files\\IS1002b.txt : Done!\n",
      "./txt_files\\IS1002c.txt : Done!\n",
      "./txt_files\\IS1002d.txt : Done!\n",
      "./txt_files\\IS1003a.txt : Done!\n",
      "./txt_files\\IS1003b.txt : Done!\n",
      "./txt_files\\IS1003c.txt : Done!\n",
      "./txt_files\\IS1003d.txt : Done!\n",
      "./txt_files\\IS1004a.txt : Done!\n",
      "./txt_files\\IS1004b.txt : Done!\n",
      "./txt_files\\IS1004c.txt : Done!\n",
      "./txt_files\\IS1004d.txt : Done!\n",
      "./txt_files\\IS1005a.txt : Done!\n",
      "./txt_files\\IS1005b.txt : Done!\n",
      "./txt_files\\IS1005c.txt : Done!\n",
      "./txt_files\\IS1006a.txt : Done!\n",
      "./txt_files\\IS1006b.txt : Done!\n",
      "./txt_files\\IS1006c.txt : Done!\n",
      "./txt_files\\IS1006d.txt : Done!\n",
      "./txt_files\\IS1007a.txt : Done!\n",
      "./txt_files\\IS1007b.txt : Done!\n",
      "./txt_files\\IS1007c.txt : Done!\n",
      "./txt_files\\IS1007d.txt : Done!\n",
      "./txt_files\\IS1008a.txt : Done!\n",
      "./txt_files\\IS1008b.txt : Done!\n",
      "./txt_files\\IS1008c.txt : Done!\n",
      "./txt_files\\IS1008d.txt : Done!\n",
      "./txt_files\\IS1009a.txt : Done!\n",
      "./txt_files\\IS1009b.txt : Done!\n",
      "./txt_files\\IS1009c.txt : Done!\n",
      "./txt_files\\IS1009d.txt : Done!\n",
      "./txt_files\\TS3003a.txt : Done!\n",
      "./txt_files\\TS3003b.txt : Done!\n",
      "./txt_files\\TS3003c.txt : Done!\n",
      "./txt_files\\TS3003d.txt : Done!\n",
      "./txt_files\\TS3004a.txt : Done!\n",
      "./txt_files\\TS3004b.txt : Done!\n",
      "./txt_files\\TS3004c.txt : Done!\n",
      "./txt_files\\TS3004d.txt : Done!\n",
      "./txt_files\\TS3005a.txt : Done!\n",
      "./txt_files\\TS3005b.txt : Done!\n",
      "./txt_files\\TS3005c.txt : Done!\n",
      "./txt_files\\TS3005d.txt : Done!\n",
      "./txt_files\\TS3006a.txt : Done!\n",
      "./txt_files\\TS3006b.txt : Done!\n",
      "./txt_files\\TS3006c.txt : Done!\n",
      "./txt_files\\TS3006d.txt : Done!\n",
      "./txt_files\\TS3007a.txt : Done!\n",
      "./txt_files\\TS3007b.txt : Done!\n",
      "./txt_files\\TS3007c.txt : Done!\n",
      "./txt_files\\TS3007d.txt : Done!\n",
      "./txt_files\\TS3008a.txt : Done!\n",
      "./txt_files\\TS3008b.txt : Done!\n",
      "./txt_files\\TS3008c.txt : Done!\n",
      "./txt_files\\TS3008d.txt : Done!\n",
      "./txt_files\\TS3009a.txt : Done!\n",
      "./txt_files\\TS3009b.txt : Done!\n",
      "./txt_files\\TS3009c.txt : Done!\n",
      "./txt_files\\TS3009d.txt : Done!\n",
      "./txt_files\\TS3010a.txt : Done!\n",
      "./txt_files\\TS3010b.txt : Done!\n",
      "./txt_files\\TS3010c.txt : Done!\n",
      "./txt_files\\TS3010d.txt : Done!\n",
      "./txt_files\\TS3011a.txt : Done!\n",
      "./txt_files\\TS3011b.txt : Done!\n",
      "./txt_files\\TS3011c.txt : Done!\n",
      "./txt_files\\TS3011d.txt : Done!\n",
      "./txt_files\\TS3012a.txt : Done!\n",
      "./txt_files\\TS3012b.txt : Done!\n",
      "./txt_files\\TS3012c.txt : Done!\n",
      "./txt_files\\TS3012d.txt : Done!\n"
     ]
    }
   ],
   "source": [
    "# for each text content stored in vocD\n",
    "for k in vocD.keys():\n",
    "    # initializing the main text for file\n",
    "    text = \"\"\n",
    "    transcript = vocD[k]\n",
    "    \n",
    "    # separates the paragraphs\n",
    "    paraList = transcript.split(\"\\n\")\n",
    "    for para in paraList:\n",
    "        line = \"\"\n",
    "        # separating each word in the paragraph\n",
    "        wList = tokenizer.tokenize(para.lower())\n",
    "        \n",
    "        # finding the unique words and counts\n",
    "        wordD = Counter(wList)\n",
    "        \n",
    "        # for each word in the current wordD keys\n",
    "        for w in wordD.keys():\n",
    "            # if the word is in vocabulary list\n",
    "            if w.lower().strip() in sortedList:\n",
    "                # add the word id and count in the vocab file\n",
    "                line = line + str(sortedList.index(w.lower())) + \":\" + str(wordD[w]) + \",\"\n",
    "        # give up the empty lines\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        # adding the line to the main text of the file\n",
    "        text = text + line[:-1] + \"\\n\"\n",
    "    # opening the file for the current txt file\n",
    "    f = open('./sparse_files/'+k.split('\\\\')[-1],'w')\n",
    "    # writing\n",
    "    f.write(text)\n",
    "    # closing the file\n",
    "    f.close()\n",
    "    print(k + \" : Done!\")\n",
    "# the code takes 3 to 4 minutes depending the machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated the Vocabulary file, Topic boundary encoded file and sparse files from the txt files generated in the previous task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Crummy. (2018). 'Beautiful Soup Documentation'. Retrieved from https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "- Python Software Foundation. (2018). \"Miscellaneous operating system interfaces\". Retrieved from https://docs.python.org/3.4/library/os.html\n",
    "- Python Software Foundation. (2018). \"collections â€” High-performance container datatypes\". Retrieved from https://docs.python.org/2/library/collections.html\n",
    "- NLTK. (2018). \"NLTK 3.3 documentation\". Retrieved from https://www.nltk.org/py-modindex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
